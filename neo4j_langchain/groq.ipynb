{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq pydantic pydantic-settings spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_settings import BaseSettings\n",
    "from groq import Groq\n",
    "from neo4j import GraphDatabase\n",
    "import spacy\n",
    "import json\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Management\n",
    "class Settings(BaseSettings):\n",
    "    GROQ_API_KEY: str\n",
    "    NEO4J_URI: str\n",
    "    NEO4J_USERNAME: str\n",
    "    NEO4J_PASSWORD: str\n",
    "    GROQ_MODEL: str = \"deepseek-r1-distill-llama-70b\"\n",
    "\n",
    "    ALLOWED_ENTITY_TYPES: List[str] = Field(default=[\n",
    "        \"concept\", \"chapter\", \"subject\", \"formula\", \n",
    "        \"diagram\", \"theorem\", \"example\", \"exercise\",\n",
    "        \"activity\", \"experiment\", \"definition\",  # Added\n",
    "        \"historical_figure\", \"scientist\", \"mathematician\",\n",
    "        \"table\", \"equation\"  # Added\n",
    "    ])\n",
    "    ALLOWED_RELATIONSHIPS: List[str] = Field(default=[\n",
    "        \"part_of\", \"related_to\", \"depends_on\", \n",
    "        \"applies_to\", \"proved_by\", \"illustrated_by\",\n",
    "        \"authored_by\", \"appears_in\", \"prerequisite_for\"\n",
    "    ])\n",
    "    \n",
    "    class Config:\n",
    "        env_file = \".env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Models\n",
    "class EntityType(str, Enum):\n",
    "    CONCEPT = \"concept\"\n",
    "    CHAPTER = \"chapter\"\n",
    "    SUBJECT = \"subject\"\n",
    "    FORMULA = \"formula\"\n",
    "    DIAGRAM = \"diagram\"\n",
    "    THEOREM = \"theorem\"\n",
    "    EXAMPLE = \"example\"\n",
    "    EXERCISE = \"exercise\"\n",
    "    ACTIVITY = \"activity\"  # Added\n",
    "    EXPERIMENT = \"experiment\"  # Added\n",
    "    DEFINITION = \"definition\"  # Added\n",
    "    HISTORICAL_FIGURE = \"historical_figure\"\n",
    "    SCIENTIST = \"scientist\"\n",
    "    MATHEMATICIAN = \"mathematician\"\n",
    "    TABLE = \"table\"  # Added\n",
    "    EQUATION = \"equation\"  # Added\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str\n",
    "    type: EntityType\n",
    "    properties: Dict[str, str] = Field(default_factory=dict)\n",
    "    \n",
    "    def validate_entity_type(cls, v):\n",
    "        if v not in Settings().ALLOWED_ENTITY_TYPES:\n",
    "            raise ValueError(f\"Invalid entity type: {v}. Allowed types: {Settings().ALLOWED_ENTITY_TYPES}\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relationship(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    type: str\n",
    "    properties: Dict[str, str] = Field(default_factory=dict)\n",
    "    \n",
    "    def validate_relationship_type(cls, v):\n",
    "        if v not in Settings().ALLOWED_RELATIONSHIPS:\n",
    "            raise ValueError(f\"Invalid relationship type: {v}. Allowed types: {Settings().ALLOWED_RELATIONSHIPS}\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphData(BaseModel):\n",
    "    entities: List[Entity]\n",
    "    relationships: List[Relationship]\n",
    "    source_text: Optional[str] = None\n",
    "    book_reference: Optional[str] = None\n",
    "    grade_level: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Graph Builder\n",
    "class NCERTKnowledgeGraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.settings = Settings()\n",
    "        self.groq_client = Groq(api_key=self.settings.GROQ_API_KEY)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.driver = GraphDatabase.driver(\n",
    "            self.settings.NEO4J_URI,\n",
    "            auth=(self.settings.NEO4J_USERNAME, self.settings.NEO4J_PASSWORD)\n",
    "        )\n",
    "    \n",
    "    def extract_kg_elements(self, text: str, context: Optional[Dict] = None) -> Dict:\n",
    "        \"\"\"Extract KG elements with schema enforcement through prompt only\"\"\"\n",
    "        prompt = self._build_prompt(text, context)\n",
    "        \n",
    "        try:\n",
    "            response = self.groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.settings.GROQ_MODEL,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.1  # Lower temperature for more consistent formatting\n",
    "            )\n",
    "            \n",
    "            kg_data = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Basic cleanup without strict validation\n",
    "            if 'relationships' in kg_data:\n",
    "                for rel in kg_data['relationships']:\n",
    "                    # Ensure 'type' field exists\n",
    "                    if 'relation' in rel:\n",
    "                        rel['type'] = rel.pop('relation')\n",
    "                    \n",
    "            if context:\n",
    "                kg_data.update(context)\n",
    "                \n",
    "            return kg_data\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"Invalid JSON response from Groq API\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to process KG data: {str(e)}\")\n",
    "    \n",
    "    def _build_prompt(self, text: str, context: Optional[Dict]) -> str:\n",
    "        \"\"\"Construct a strict prompt that enforces the schema through instructions\"\"\"\n",
    "        context_info = \"\"\n",
    "        if context:\n",
    "            context_info = (\n",
    "                f\"\\nAdditional Context:\\n\"\n",
    "                f\"- Book: {context.get('book_reference', 'unknown')}\\n\"\n",
    "                f\"- Grade: {context.get('grade_level', 'unknown')}\\n\"\n",
    "                f\"- Subject: {context.get('subject', 'unknown')}\\n\"\n",
    "            )\n",
    "        \n",
    "        return f\"\"\"\n",
    "        Extract knowledge graph elements from NCERT textbook content following these STRICT rules:\n",
    "\n",
    "        1. OUTPUT FORMAT (MUST follow exactly):\n",
    "        {{\n",
    "            \"entities\": [\n",
    "                {{\n",
    "                    \"name\": \"entity_name\",\n",
    "                    \"type\": \"allowed_type\",\n",
    "                    \"properties\": {{\n",
    "                        \"key\": \"value\"\n",
    "                    }}\n",
    "                }}\n",
    "            ],\n",
    "            \"relationships\": [\n",
    "                {{\n",
    "                    \"source\": \"source_entity_name\",\n",
    "                    \"target\": \"target_entity_name\",\n",
    "                    \"type\": \"relationship_type\",\n",
    "                    \"properties\": {{\n",
    "                        \"key\": \"value\"\n",
    "                    }}\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "\n",
    "        2. ENTITY TYPES (must use exactly these):\n",
    "        - concept, chapter, subject, formula, diagram, theorem, \n",
    "        - example, exercise, activity, experiment, definition,\n",
    "        - historical_figure, scientist, mathematician, table, equation\n",
    "\n",
    "        3. RELATIONSHIP TYPES (must use exactly these):\n",
    "        - part_of, related_to, depends_on, applies_to, proved_by,\n",
    "        - illustrated_by, authored_by, appears_in, prerequisite_for,\n",
    "        - instance_of, method_of, component_of, example_of\n",
    "\n",
    "        4. SPECIAL INSTRUCTIONS:\n",
    "        - Always include both \"source\" and \"target\" in relationships\n",
    "        - Use \"type\" field for relationship type (not \"relation\")\n",
    "        - For questions → type=\"exercise\"\n",
    "        - For figures/diagrams → type=\"diagram\"\n",
    "        - For math expressions → type=\"formula\"\n",
    "\n",
    "        {context_info}\n",
    "\n",
    "        Textbook content to analyze:\n",
    "        {text}\n",
    "\n",
    "        Return ONLY the JSON output matching the exact format above.\n",
    "        \n",
    "            Extract knowledge graph elements with these additional rules:\n",
    "    \n",
    "        5. LABEL SIMPLIFICATION RULES:\n",
    "        - Use simple, single-word labels where possible\n",
    "        - For complex concepts: \n",
    "            - \"chemical compound\" → \"compound\"\n",
    "            - \"periodic table element\" → \"element\"\n",
    "            - \"scientific phenomenon\" → \"phenomenon\"\n",
    "        - Replace spaces with underscores\n",
    "        \"\"\"\n",
    "    \n",
    "    def store_in_neo4j(self, kg_data: Dict):\n",
    "        \"\"\"Store extracted knowledge graph in Neo4j\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Create entities (nodes)\n",
    "            for entity in kg_data.get(\"entities\", []):\n",
    "                session.execute_write(\n",
    "                    self._create_entity_node,\n",
    "                    entity.get(\"name\", \"\"),\n",
    "                    entity.get(\"type\", \"concept\"),  # Default to 'concept' if missing\n",
    "                    entity.get(\"properties\", {})\n",
    "                )\n",
    "            \n",
    "            # Create relationships\n",
    "            for rel in kg_data.get(\"relationships\", []):\n",
    "                session.execute_write(\n",
    "                    self._create_relationship,\n",
    "                    rel.get(\"source\", \"\"),\n",
    "                    rel.get(\"target\", \"\"),\n",
    "                    rel.get(\"type\", \"related_to\"),  # Default to 'related_to' if missing\n",
    "                    rel.get(\"properties\", {})\n",
    "                )\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if kg_data.get(\"source_text\") or kg_data.get(\"book_reference\"):\n",
    "                session.execute_write(\n",
    "                    self._add_metadata,\n",
    "                    kg_data.get(\"source_text\"),\n",
    "                    kg_data.get(\"book_reference\"),\n",
    "                    kg_data.get(\"grade_level\")\n",
    "                )\n",
    "    \n",
    "    def _create_entity_node(self, tx, name: str, label: str, properties: Dict):\n",
    "        \"\"\"Create a node with sanitized inputs\"\"\"\n",
    "        # Sanitize inputs\n",
    "        safe_label = self._sanitize_neo4j_input(label)\n",
    "        safe_name = self._sanitize_neo4j_input(name)\n",
    "        \n",
    "        # Sanitize property keys and values\n",
    "        safe_properties = {\n",
    "            self._sanitize_neo4j_input(k): self._sanitize_neo4j_input(str(v))\n",
    "            for k, v in properties.items()\n",
    "        }\n",
    "        \n",
    "        props_str = \", \".join([f\"{k}: ${k}\" for k in safe_properties.keys()])\n",
    "        query = (\n",
    "            f\"MERGE (n:`{safe_label}` {{name: $name\"\n",
    "            f\"{', ' + props_str if props_str else ''}}})\"\n",
    "            f\" SET n.created_at = datetime()\"\n",
    "        )\n",
    "        params = {\"name\": safe_name, **safe_properties}\n",
    "        tx.run(query, **params)\n",
    "\n",
    "    def _create_relationship(self, tx, source: str, target: str, rel_type: str, properties: Dict):\n",
    "        \"\"\"Create a relationship with sanitized inputs\"\"\"\n",
    "        safe_rel_type = self._sanitize_neo4j_input(rel_type)\n",
    "        safe_source = self._sanitize_neo4j_input(source)\n",
    "        safe_target = self._sanitize_neo4j_input(target)\n",
    "        \n",
    "        safe_properties = {\n",
    "            self._sanitize_neo4j_input(k): self._sanitize_neo4j_input(str(v))\n",
    "            for k, v in properties.items()\n",
    "        }\n",
    "        \n",
    "        props_str = \", \".join([f\"{k}: ${k}\" for k in safe_properties.keys()])\n",
    "        query = (\n",
    "            f\"MATCH (a), (b) \"\n",
    "            f\"WHERE a.name = $source AND b.name = $target \"\n",
    "            f\"MERGE (a)-[r:`{safe_rel_type}` \"\n",
    "            f\"{'{' + props_str + '}' if props_str else ''}]->(b)\"\n",
    "            f\" SET r.created_at = datetime()\"\n",
    "        )\n",
    "        params = {\"source\": safe_source, \"target\": safe_target, **safe_properties}\n",
    "        tx.run(query, **params)\n",
    "        \n",
    "    def _add_metadata(self, tx, source_text: Optional[str], book_ref: Optional[str], grade: Optional[str]):\n",
    "        \"\"\"Add processing metadata to the graph\"\"\"\n",
    "        if book_ref:\n",
    "            tx.run(\n",
    "                \"MERGE (m:Metadata {book_reference: $book_ref}) \"\n",
    "                \"SET m.last_updated = datetime(), \"\n",
    "                \"m.grade_level = $grade, \"\n",
    "                \"m.processed = true\",\n",
    "                book_ref=book_ref,\n",
    "                grade=grade\n",
    "            )\n",
    "    \n",
    "    def process_material(self, text: str, context: Optional[Dict] = None) -> Dict:\n",
    "        \"\"\"Full processing pipeline that returns raw dictionary\"\"\"\n",
    "        # Preprocessing\n",
    "        doc = self.nlp(text)\n",
    "        clean_text = \" \".join([\n",
    "            token.lemma_.lower() for token in doc \n",
    "            if not token.is_stop and not token.is_punct\n",
    "        ])\n",
    "        \n",
    "        # Knowledge extraction\n",
    "        kg_data = self.extract_kg_elements(clean_text, context)\n",
    "        \n",
    "        # Ensure basic structure exists\n",
    "        kg_data.setdefault(\"entities\", [])\n",
    "        kg_data.setdefault(\"relationships\", [])\n",
    "        \n",
    "        # Store in Neo4j\n",
    "        self.store_in_neo4j(kg_data)\n",
    "        \n",
    "        return kg_data\n",
    "    \n",
    "    def load_and_process_pdf(self, file_path: str, context: Dict) -> KnowledgeGraphData:\n",
    "        \"\"\"Load and process a single NCERT PDF file\"\"\"\n",
    "        try:\n",
    "            # Load PDF\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            pages = loader.load()\n",
    "            \n",
    "            # Split text into manageable chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len\n",
    "            )\n",
    "            chunks = text_splitter.split_documents(pages)\n",
    "            \n",
    "            # Process each chunk\n",
    "            results = []\n",
    "            for chunk in chunks:\n",
    "                result = self.process_material(chunk.page_content, context)\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing PDF {file_path}: {str(e)}\")\n",
    "\n",
    "    def _sanitize_neo4j_input(self, input_str: str) -> str:\n",
    "        \"\"\"Sanitize strings for Neo4j labels and properties\"\"\"\n",
    "        if not input_str:\n",
    "            return \"unknown\"\n",
    "        \n",
    "        # Remove special characters and replace spaces\n",
    "        sanitized = \"\".join(\n",
    "            c if c.isalnum() or c in \"_-\" else \"_\" \n",
    "            for c in input_str.strip()\n",
    "        )\n",
    "        \n",
    "        # Ensure it starts with a letter\n",
    "        if sanitized and not sanitized[0].isalpha():\n",
    "            sanitized = \"e_\" + sanitized\n",
    "            \n",
    "        return sanitized[:64]  # Limit length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = NCERTKnowledgeGraphBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed chunk 1:\n",
      "Entities found: 9\n",
      "Relationships found: 10\n",
      "- chemical_reactions_equations (chapter)\n",
      "- milk_room_temperature (example)\n",
      "- iron_tawa_humid_atmosphere (example)\n",
      "\n",
      "Processed chunk 2:\n",
      "Entities found: 10\n",
      "Relationships found: 10\n",
      "- figure_1.1 (diagram)\n",
      "- activity_1.1 (activity)\n",
      "- magnesium_ribbon_burning (concept)\n",
      "\n",
      "Processed chunk 3:\n",
      "Entities found: 5\n",
      "Relationships found: 5\n",
      "- activity_1.2 (activity)\n",
      "- activity_1.3 (activity)\n",
      "- figure_1.2 (diagram)\n",
      "\n",
      "Processed chunk 4:\n",
      "Entities found: 7\n",
      "Relationships found: 4\n",
      "- magnesium (element)\n",
      "- oxygen (element)\n",
      "- magnesium_oxide (compound)\n",
      "\n",
      "Processed chunk 5:\n",
      "Entities found: 5\n",
      "Relationships found: 4\n",
      "- word_equation (concept)\n",
      "- magnesium (element)\n",
      "- oxygen (element)\n",
      "\n",
      "Processed chunk 6:\n",
      "Entities found: 6\n",
      "Relationships found: 5\n",
      "- chemical_equation (concept)\n",
      "- chemical_formula (concept)\n",
      "- magnesium (element)\n",
      "\n",
      "Processed chunk 7:\n",
      "Entities found: 10\n",
      "Relationships found: 9\n",
      "- balanced_chemical_equations (concept)\n",
      "- law_conservation_mass (concept)\n",
      "- chemical_equation (formula)\n",
      "\n",
      "Processed chunk 8:\n",
      "Entities found: 10\n",
      "Relationships found: 10\n",
      "- chemical_equation (concept)\n",
      "- equation_1.3 (equation)\n",
      "- equation_1.4 (equation)\n",
      "\n",
      "Processed chunk 9:\n",
      "Entities found: 7\n",
      "Relationships found: 7\n",
      "- iron (formula)\n",
      "- water (formula)\n",
      "- iron_ii_iii_oxide (formula)\n",
      "\n",
      "Processed chunk 10:\n",
      "Entities found: 5\n",
      "Relationships found: 5\n",
      "- Fe (element)\n",
      "- H2O (compound)\n",
      "- Fe3O4 (compound)\n",
      "\n",
      "Processed chunk 11:\n",
      "Entities found: 6\n",
      "Relationships found: 5\n",
      "- fe (compound)\n",
      "- h2o (compound)\n",
      "- fe3o4 (compound)\n",
      "\n",
      "Processed chunk 12:\n",
      "Entities found: 7\n",
      "Relationships found: 6\n",
      "- balanced_chemical_equation (equation)\n",
      "- physical_state (concept)\n",
      "- equation_1.9 (equation)\n",
      "\n",
      "Processed chunk 13:\n",
      "Entities found: 9\n",
      "Relationships found: 7\n",
      "- Chemical_Equation (concept)\n",
      "- Balanced_Equation (concept)\n",
      "- CO(g)_Reaction (example)\n",
      "\n",
      "Processed chunk 14:\n",
      "Entities found: 7\n",
      "Relationships found: 6\n",
      "- chemical_reaction (concept)\n",
      "- calcium_oxide (element)\n",
      "- water (element)\n",
      "\n",
      "Processed chunk 15:\n",
      "Entities found: 12\n",
      "Relationships found: 6\n",
      "- figure_1_3 (diagram)\n",
      "- calcium_oxide (compound)\n",
      "- water (compound)\n",
      "\n",
      "Processed chunk 16:\n",
      "Entities found: 10\n",
      "Relationships found: 9\n",
      "- aluminium_chloride (compound)\n",
      "- sodium (element)\n",
      "- water (compound)\n",
      "\n",
      "Processed chunk 17:\n",
      "Entities found: 7\n",
      "Relationships found: 5\n",
      "- combination_reaction (concept)\n",
      "- exothermic_reaction (concept)\n",
      "- activity_1.4 (activity)\n",
      "\n",
      "Processed chunk 18:\n",
      "Entities found: 14\n",
      "Relationships found: 8\n",
      "- energy (concept)\n",
      "- food (concept)\n",
      "- carbohydrate (concept)\n",
      "\n",
      "Processed chunk 19:\n",
      "Entities found: 6\n",
      "Relationships found: 3\n",
      "- slaked_lime (compound)\n",
      "- whitewash (compound)\n",
      "- calcium_hydroxide (compound)\n",
      "\n",
      "Processed chunk 20:\n",
      "Entities found: 7\n",
      "Relationships found: 5\n",
      "- figure_1.4 (diagram)\n",
      "- figure_1.5 (diagram)\n",
      "- activity_1.6 (activity)\n",
      "\n",
      "Processed chunk 21:\n",
      "Entities found: 10\n",
      "Relationships found: 7\n",
      "- Spirit_Lamp_Burner (experiment)\n",
      "- Ferrous_Sulphate (compound)\n",
      "- Ferric_Oxide (compound)\n",
      "\n",
      "Processed chunk 22:\n",
      "Entities found: 11\n",
      "Relationships found: 8\n",
      "- calcium_oxide (compound)\n",
      "- lime (compound)\n",
      "- quick_lime (compound)\n",
      "\n",
      "Processed chunk 23:\n",
      "Entities found: 17\n",
      "Relationships found: 15\n",
      "- chemical_reactions (concept)\n",
      "- equations (concept)\n",
      "- activity_1.7 (activity)\n",
      "\n",
      "Processed chunk 24:\n",
      "Entities found: 11\n",
      "Relationships found: 10\n",
      "- electrolysis (concept)\n",
      "- electrode (equipment)\n",
      "- battery (equipment)\n",
      "\n",
      "Processed chunk 25:\n",
      "Entities found: 4\n",
      "Relationships found: 4\n",
      "- figure_1.6 (diagram)\n",
      "- 2pb(no3)2(s)_heat (formula)\n",
      "- activity_1.7 (activity)\n",
      "\n",
      "Processed chunk 26:\n",
      "Entities found: 6\n",
      "Relationships found: 4\n",
      "- figure_1.8 (diagram)\n",
      "- exercise_1 (exercise)\n",
      "- exercise_2 (exercise)\n",
      "\n",
      "Processed chunk 27:\n",
      "Entities found: 10\n",
      "Relationships found: 4\n",
      "- iron_nail (compound)\n",
      "- copper_sulphate_solution (compound)\n",
      "- barium_hydroxide (compound)\n",
      "\n",
      "Processed chunk 28:\n",
      "Entities found: 10\n",
      "Relationships found: 6\n",
      "- reaction (concept)\n",
      "- black_white_photography (concept)\n",
      "- energy (concept)\n",
      "\n",
      "Processed chunk 29:\n",
      "Entities found: 16\n",
      "Relationships found: 9\n",
      "- displacement_reaction (concept)\n",
      "- double_displacement_reaction (concept)\n",
      "- FeSO4 (compound)\n",
      "\n",
      "Processed chunk 30:\n",
      "Entities found: 6\n",
      "Relationships found: 6\n",
      "- activity_1.10 (activity)\n",
      "- sodium_sulphate (compound)\n",
      "- barium_chloride (compound)\n",
      "\n",
      "Processed chunk 31:\n",
      "Entities found: 8\n",
      "Relationships found: 8\n",
      "- oxidation (concept)\n",
      "- reduction (concept)\n",
      "- copper (element)\n",
      "\n",
      "Processed chunk 32:\n",
      "Entities found: 7\n",
      "Relationships found: 6\n",
      "- oxidation (concept)\n",
      "- reduction (concept)\n",
      "- redox_reaction (concept)\n",
      "\n",
      "Processed chunk 33:\n",
      "Entities found: 4\n",
      "Relationships found: 5\n",
      "- BaSO4 (compound)\n",
      "- NaCl (compound)\n",
      "- Ba(NO3)2 (compound)\n",
      "\n",
      "Processed chunk 34:\n",
      "Entities found: 8\n",
      "Relationships found: 6\n",
      "- oxidation (concept)\n",
      "- reduction (concept)\n",
      "- reaction_1.31 (reaction)\n",
      "\n",
      "Processed chunk 35:\n",
      "Entities found: 9\n",
      "Relationships found: 8\n",
      "- corrosion (concept)\n",
      "- iron (element)\n",
      "- copper (element)\n",
      "\n",
      "Processed chunk 36:\n",
      "Entities found: 14\n",
      "Relationships found: 9\n",
      "- iron (concept)\n",
      "- corrosion (concept)\n",
      "- chapter_3 (chapter)\n",
      "\n",
      "Processed chunk 37:\n",
      "Entities found: 7\n",
      "Relationships found: 6\n",
      "- Chemical_Equation (concept)\n",
      "- Combination_Reaction (concept)\n",
      "- Decomposition_Reaction (concept)\n",
      "\n",
      "Processed chunk 38:\n",
      "Entities found: 11\n",
      "Relationships found: 2\n",
      "- displacement_reaction (concept)\n",
      "- double_displacement_reaction (concept)\n",
      "- precipitation_reaction (concept)\n",
      "\n",
      "Processed chunk 39:\n",
      "Entities found: 7\n",
      "Relationships found: 8\n",
      "- Chemical_Reactions (chapter)\n",
      "- Exercise_3 (exercise)\n",
      "- Exercise_4 (exercise)\n",
      "\n",
      "Processed chunk 40:\n",
      "Entities found: 12\n",
      "Relationships found: 10\n",
      "- Science (subject)\n",
      "- Chemical_Reactions (chapter)\n",
      "- Reaction_1 (example)\n",
      "\n",
      "Processed chunk 41:\n",
      "Entities found: 16\n",
      "Relationships found: 12\n",
      "- potassium_iodide (compound)\n",
      "- barium_bromide (compound)\n",
      "- zinc_carbonate (compound)\n",
      "\n",
      "Processed chunk 42:\n",
      "Entities found: 14\n",
      "Relationships found: 8\n",
      "- Group_Activity (activity)\n",
      "- Beaker_B (equipment)\n",
      "- Beaker_C (equipment)\n",
      "\n",
      "Processed chunk 43:\n",
      "Entities found: 11\n",
      "Relationships found: 6\n",
      "- refining_silver (concept)\n",
      "- silver_recovery (concept)\n",
      "- silver_nitrate_solution (compound)\n"
     ]
    }
   ],
   "source": [
    "context = {\n",
    "    \"book_reference\": \"NCERT Science Class 10\",\n",
    "    \"grade_level\": \"10\",\n",
    "    \"subject\": \"Science\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Process a single PDF file\n",
    "    pdf_path = \"./books/ncert_class_x_science_chapter_1.pdf\"\n",
    "    results = builder.load_and_process_pdf(pdf_path, context)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nProcessed chunk {i+1}:\")\n",
    "        print(\"Entities found:\", len(result.get(\"entities\", [])))\n",
    "        print(\"Relationships found:\", len(result.get(\"relationships\", [])))\n",
    "        \n",
    "        # Example of accessing data\n",
    "        for entity in result.get(\"entities\", [])[:3]:  # Print first 3 entities\n",
    "            print(f\"- {entity.get('name')} ({entity.get('type')})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing material: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process a single PDF file\n",
    "    pdf_path = \"./books/ncert_class_x_science_chapter_2.pdf\"\n",
    "    results = builder.load_and_process_pdf(pdf_path, context)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nProcessed chunk {i+1}:\")\n",
    "        print(\"Entities found:\", len(result.get(\"entities\", [])))\n",
    "        print(\"Relationships found:\", len(result.get(\"relationships\", [])))\n",
    "        \n",
    "        # Example of accessing data\n",
    "        for entity in result.get(\"entities\", [])[:3]:  # Print first 3 entities\n",
    "            print(f\"- {entity.get('name')} ({entity.get('type')})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing material: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process a single PDF file\n",
    "    pdf_path = \"./books/ncert_class_x_science_chapter_3.pdf\"\n",
    "    results = builder.load_and_process_pdf(pdf_path, context)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nProcessed chunk {i+1}:\")\n",
    "        print(\"Entities found:\", len(result.get(\"entities\", [])))\n",
    "        print(\"Relationships found:\", len(result.get(\"relationships\", [])))\n",
    "        \n",
    "        # Example of accessing data\n",
    "        for entity in result.get(\"entities\", [])[:3]:  # Print first 3 entities\n",
    "            print(f\"- {entity.get('name')} ({entity.get('type')})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing material: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
